#!/usr/bin/env python3
"""
Video Analyzer - ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ Ù…Ø¹ Ø§Ù„ØµÙˆØª ÙˆØ§Ù„ØµÙˆØ±Ø©
ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø§Ù„Ø«Ø§Ù†ÙŠØ© ÙˆÙŠÙˆÙ„Ø¯ ØªÙ‚Ø±ÙŠØ± Markdown Ù…ÙØµÙ„
"""

import cv2
import whisper
import requests
import os
import json
import numpy as np
from datetime import timedelta
from pathlib import Path
import tempfile
import subprocess
import sys
from PIL import Image
import base64
from io import BytesIO

class VideoAnalyzer:
    def __init__(self, video_url, output_dir="video_analysis"):
        self.video_url = video_url
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.video_path = None
        self.audio_path = None
        self.whisper_model = None
        
    def download_video(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù…Ù† URL"""
        print("ğŸ“¥ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ...")
        try:
            response = requests.get(self.video_url, stream=True, timeout=60)
            response.raise_for_status()
            
            self.video_path = self.output_dir / "video.mp4"
            with open(self.video_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ: {self.video_path}")
            return True
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ: {e}")
            return False
    
    def extract_audio(self):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØª Ù…Ù† Ø§Ù„ÙÙŠØ¯ÙŠÙˆ"""
        print("ğŸµ Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØª...")
        try:
            self.audio_path = self.output_dir / "audio.wav"
            
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… opencv Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØª
            cap = cv2.VideoCapture(str(self.video_path))
            fps = cap.get(cv2.CAP_PROP_FPS)
            cap.release()
            
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… ffmpeg-python
            import ffmpeg
            (
                ffmpeg
                .input(str(self.video_path))
                .output(str(self.audio_path), acodec='pcm_s16le', ac=1, ar='16k')
                .overwrite_output()
                .run(capture_stdout=True, capture_stderr=True)
            )
            
            print(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØª: {self.audio_path}")
            return True
        except Exception as e:
            print(f"âš ï¸ ØªØ­Ø°ÙŠØ±: Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØª: {e}")
            return False
    
    def transcribe_audio(self):
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Whisper"""
        if not self.audio_path or not self.audio_path.exists():
            print("âš ï¸ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ù„Ù ØµÙˆØªÙŠ Ù„Ù„ØªØ­ÙˆÙŠÙ„")
            return None
        
        print("ğŸ¤ Ø¬Ø§Ø±ÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ (Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ø¨Ø¹Ø¶ Ø§Ù„ÙˆÙ‚Øª)...")
        try:
            # ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Whisper (base Ù„Ù„Ø³Ø±Ø¹Ø©)
            if self.whisper_model is None:
                print("ğŸ“¦ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Whisper...")
                self.whisper_model = whisper.load_model("base")
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ
            result = self.whisper_model.transcribe(
                str(self.audio_path),
                language="en",  # ÙŠÙ…ÙƒÙ† ØªØºÙŠÙŠØ±Ù‡ Ø­Ø³Ø¨ Ù„ØºØ© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
                verbose=False
            )
            
            print(f"âœ… ØªÙ… ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ ({len(result['segments'])} Ù…Ù‚Ø·Ø¹)")
            return result
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª: {e}")
            return None
    
    def analyze_frames(self, interval=1.0):
        """ØªØ­Ù„ÙŠÙ„ Ø¥Ø·Ø§Ø±Ø§Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨ÙØ§ØµÙ„ Ø²Ù…Ù†ÙŠ Ù…Ø­Ø¯Ø¯"""
        print(f"ğŸ¬ Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ø¥Ø·Ø§Ø±Ø§Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆ (ÙƒÙ„ {interval} Ø«Ø§Ù†ÙŠØ©)...")
        
        cap = cv2.VideoCapture(str(self.video_path))
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps
        
        frames_data = []
        frame_interval = int(fps * interval)
        
        frame_count = 0
        saved_count = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            if frame_count % frame_interval == 0:
                timestamp = frame_count / fps
                
                # Ø­ÙØ¸ Ø§Ù„Ø¥Ø·Ø§Ø±
                frame_filename = f"frame_{saved_count:04d}_{timestamp:.2f}s.jpg"
                frame_path = self.output_dir / "frames" / frame_filename
                frame_path.parent.mkdir(exist_ok=True)
                
                cv2.imwrite(str(frame_path), frame)
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø·Ø§Ø±
                frame_info = self.analyze_single_frame(frame, timestamp)
                frame_info['filename'] = frame_filename
                frame_info['path'] = str(frame_path)
                
                frames_data.append(frame_info)
                saved_count += 1
                
                if saved_count % 10 == 0:
                    print(f"  ğŸ“¸ ØªÙ… ØªØ­Ù„ÙŠÙ„ {saved_count} Ø¥Ø·Ø§Ø±...")
            
            frame_count += 1
        
        cap.release()
        print(f"âœ… ØªÙ… ØªØ­Ù„ÙŠÙ„ {len(frames_data)} Ø¥Ø·Ø§Ø±")
        
        return {
            'fps': fps,
            'total_frames': total_frames,
            'duration': duration,
            'frames': frames_data
        }
    
    def analyze_single_frame(self, frame, timestamp):
        """ØªØ­Ù„ÙŠÙ„ Ø¥Ø·Ø§Ø± ÙˆØ§Ø­Ø¯"""
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©
        height, width, channels = frame.shape
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø³Ø·ÙˆØ¹ Ø§Ù„Ù…ØªÙˆØ³Ø·
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        brightness = np.mean(gray)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¨Ø§ÙŠÙ†
        contrast = np.std(gray)
        
        # Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø§Ù„Ø³Ø§Ø¦Ø¯Ø©
        avg_color = np.mean(frame, axis=(0, 1))
        
        # ÙƒØ´Ù Ø§Ù„Ø­ÙˆØ§Ù (Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ÙÙƒØ±Ø© Ø¹Ù† Ø§Ù„ØªØ¹Ù‚ÙŠØ¯)
        edges = cv2.Canny(gray, 100, 200)
        edge_density = np.sum(edges > 0) / (width * height)
        
        return {
            'timestamp': timestamp,
            'time_formatted': str(timedelta(seconds=int(timestamp))),
            'resolution': f"{width}x{height}",
            'brightness': float(brightness),
            'contrast': float(contrast),
            'avg_color_bgr': [float(c) for c in avg_color],
            'edge_density': float(edge_density),
            'scene_complexity': 'high' if edge_density > 0.1 else 'medium' if edge_density > 0.05 else 'low'
        }
    
    def generate_markdown_report(self, frames_analysis, transcription):
        """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Markdown Ø´Ø§Ù…Ù„"""
        print("ğŸ“ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Markdown...")
        
        md_content = []
        
        # Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
        md_content.append("# ğŸ“¹ ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ø´Ø§Ù…Ù„\n")
        md_content.append(f"**Video Analysis Report - Complete Training Material**\n\n")
        md_content.append(f"---\n\n")
        
        # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ø§Ù…Ø©
        md_content.append("## ğŸ“Š Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ø¹Ø§Ù…Ø©\n\n")
        md_content.append(f"- **Ø§Ù„Ù…ØµØ¯Ø±**: {self.video_url}\n")
        md_content.append(f"- **Ø§Ù„Ù…Ø¯Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©**: {timedelta(seconds=int(frames_analysis['duration']))}\n")
        md_content.append(f"- **Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª (FPS)**: {frames_analysis['fps']:.2f}\n")
        md_content.append(f"- **Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª**: {frames_analysis['total_frames']}\n")
        md_content.append(f"- **Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø­Ù„Ù„Ø©**: {len(frames_analysis['frames'])}\n\n")
        
        # Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬ Ù…Ù† Ø§Ù„ØµÙˆØª
        if transcription:
            md_content.append("## ğŸ¤ Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ Ù…Ù† Ø§Ù„ØµÙˆØª (Transcription)\n\n")
            md_content.append("### Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„:\n\n")
            md_content.append(f"```\n{transcription['text']}\n```\n\n")
            
            md_content.append("### Ø§Ù„Ù†Øµ Ù…Ù‚Ø³Ù… Ø­Ø³Ø¨ Ø§Ù„ÙˆÙ‚Øª:\n\n")
            for segment in transcription['segments']:
                start_time = str(timedelta(seconds=int(segment['start'])))
                end_time = str(timedelta(seconds=int(segment['end'])))
                md_content.append(f"**[{start_time} - {end_time}]**\n")
                md_content.append(f"> {segment['text'].strip()}\n\n")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø¨Ø§Ù„ØªÙØµÙŠÙ„
        md_content.append("## ğŸ¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø§Ù„Ø«Ø§Ù†ÙŠØ© (Frame-by-Frame Analysis)\n\n")
        
        for i, frame in enumerate(frames_analysis['frames']):
            md_content.append(f"### â±ï¸ Ø§Ù„Ø«Ø§Ù†ÙŠØ© {frame['timestamp']:.2f} ({frame['time_formatted']})\n\n")
            
            # Ø§Ù„ØµÙˆØ±Ø©
            md_content.append(f"![Frame {i}](frames/{frame['filename']})\n\n")
            
            # Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„ØªÙ‚Ù†ÙŠØ©
            md_content.append("**Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„ØªÙ‚Ù†ÙŠØ©:**\n\n")
            md_content.append(f"- **Ø§Ù„Ø¯Ù‚Ø©**: {frame['resolution']}\n")
            md_content.append(f"- **Ø§Ù„Ø³Ø·ÙˆØ¹**: {frame['brightness']:.2f}/255\n")
            md_content.append(f"- **Ø§Ù„ØªØ¨Ø§ÙŠÙ†**: {frame['contrast']:.2f}\n")
            md_content.append(f"- **ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù…Ø´Ù‡Ø¯**: {frame['scene_complexity']}\n")
            md_content.append(f"- **ÙƒØ«Ø§ÙØ© Ø§Ù„Ø­ÙˆØ§Ù**: {frame['edge_density']:.4f}\n\n")
            
            # Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„ Ù…Ù† Ø§Ù„ØµÙˆØª
            if transcription:
                matching_text = self.find_matching_transcription(
                    frame['timestamp'], 
                    transcription['segments']
                )
                if matching_text:
                    md_content.append("**Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù†Ø·ÙˆÙ‚ ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù„Ø­Ø¸Ø©:**\n\n")
                    md_content.append(f"> {matching_text}\n\n")
            
            md_content.append("---\n\n")
        
        # Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        md_content.append("## ğŸ“ˆ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„\n\n")
        
        avg_brightness = np.mean([f['brightness'] for f in frames_analysis['frames']])
        avg_contrast = np.mean([f['contrast'] for f in frames_analysis['frames']])
        
        md_content.append(f"- **Ù…ØªÙˆØ³Ø· Ø§Ù„Ø³Ø·ÙˆØ¹**: {avg_brightness:.2f}/255\n")
        md_content.append(f"- **Ù…ØªÙˆØ³Ø· Ø§Ù„ØªØ¨Ø§ÙŠÙ†**: {avg_contrast:.2f}\n")
        
        complexity_counts = {}
        for f in frames_analysis['frames']:
            complexity = f['scene_complexity']
            complexity_counts[complexity] = complexity_counts.get(complexity, 0) + 1
        
        md_content.append(f"- **ØªÙˆØ²ÙŠØ¹ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯**:\n")
        for complexity, count in complexity_counts.items():
            percentage = (count / len(frames_analysis['frames'])) * 100
            md_content.append(f"  - {complexity}: {count} Ø¥Ø·Ø§Ø± ({percentage:.1f}%)\n")
        
        md_content.append("\n---\n\n")
        md_content.append("*ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù‡Ø°Ø§ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¨ÙˆØ§Ø³Ø·Ø© Video Analyzer*\n")
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        report_path = self.output_dir / "video_analysis_report.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(''.join(md_content))
        
        print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {report_path}")
        return report_path
    
    def find_matching_transcription(self, timestamp, segments):
        """Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚ Ù„Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø­Ø¯Ø¯"""
        for segment in segments:
            if segment['start'] <= timestamp <= segment['end']:
                return segment['text'].strip()
        return None
    
    def analyze(self, frame_interval=1.0):
        """ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„"""
        print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ...\n")
        
        # 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
        if not self.download_video():
            return False
        
        # 2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØª
        audio_extracted = self.extract_audio()
        
        # 3. ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ
        transcription = None
        if audio_extracted:
            transcription = self.transcribe_audio()
        
        # 4. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª
        frames_analysis = self.analyze_frames(interval=frame_interval)
        
        # 5. ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        report_path = self.generate_markdown_report(frames_analysis, transcription)
        
        # 6. Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù…
        data_path = self.output_dir / "analysis_data.json"
        with open(data_path, 'w', encoding='utf-8') as f:
            json.dump({
                'video_url': self.video_url,
                'frames_analysis': frames_analysis,
                'transcription': transcription
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­!")
        print(f"ğŸ“ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©:")
        print(f"   - Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {report_path}")
        print(f"   - Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù…: {data_path}")
        print(f"   - Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª: {self.output_dir / 'frames'}")
        
        return True


def main():
    if len(sys.argv) < 2:
        print("Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…: python video_analyzer.py <video_url> [frame_interval]")
        print("Ù…Ø«Ø§Ù„: python video_analyzer.py https://example.com/video.mp4 1.0")
        sys.exit(1)
    
    video_url = sys.argv[1]
    frame_interval = float(sys.argv[2]) if len(sys.argv) > 2 else 1.0
    
    analyzer = VideoAnalyzer(video_url)
    analyzer.analyze(frame_interval=frame_interval)


if __name__ == "__main__":
    main()
