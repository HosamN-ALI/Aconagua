#!/usr/bin/env python3
"""
Video Analyzer Enhanced - ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ Ù…Ø¹ Ø§Ù„ØµÙˆØª ÙˆØ§Ù„ØµÙˆØ±Ø©
ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø§Ù„Ø«Ø§Ù†ÙŠØ© ÙˆÙŠÙˆÙ„Ø¯ ØªÙ‚Ø±ÙŠØ± Markdown Ù…ÙØµÙ„
Ù†Ø³Ø®Ø© Ù…Ø­Ø³Ù†Ø© Ù…Ø¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØµÙˆØª Ø¨Ø¯ÙŠÙ„
"""

import cv2
import requests
import os
import json
import numpy as np
from datetime import timedelta
from pathlib import Path
import tempfile
import subprocess
import sys
from PIL import Image

class VideoAnalyzer:
    def __init__(self, video_url, output_dir="video_analysis"):
        self.video_url = video_url
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.video_path = None
        self.audio_path = None
        self.whisper_model = None
        
    def download_video(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù…Ù† URL"""
        print("ğŸ“¥ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ...")
        try:
            response = requests.get(self.video_url, stream=True, timeout=120)
            response.raise_for_status()
            
            self.video_path = self.output_dir / "video.mp4"
            total_size = 0
            with open(self.video_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
                    total_size += len(chunk)
                    if total_size % (1024*1024) == 0:  # ÙƒÙ„ 1MB
                        print(f"  ØªÙ… ØªØ­Ù…ÙŠÙ„ {total_size / (1024*1024):.1f} MB...")
            
            print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ: {self.video_path} ({total_size / (1024*1024):.2f} MB)")
            return True
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ: {e}")
            return False
    
    def extract_audio_subprocess(self):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… subprocess"""
        print("ğŸµ Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØª...")
        try:
            self.audio_path = self.output_dir / "audio.wav"
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… ffmpeg Ù…Ø¨Ø§Ø´Ø±Ø©
            cmd = [
                'ffmpeg', '-i', str(self.video_path),
                '-vn', '-acodec', 'pcm_s16le',
                '-ar', '16000', '-ac', '1',
                str(self.audio_path), '-y'
            ]
            
            result = subprocess.run(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                timeout=300
            )
            
            if result.returncode == 0 and self.audio_path.exists():
                print(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØª: {self.audio_path}")
                return True
            else:
                print(f"âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØª (ffmpeg ØºÙŠØ± Ù…ØªÙˆÙØ±)")
                return False
                
        except FileNotFoundError:
            print("âš ï¸ ffmpeg ØºÙŠØ± Ù…Ø«Ø¨Øª - Ø³ÙŠØªÙ… ØªØ®Ø·ÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØª")
            return False
        except Exception as e:
            print(f"âš ï¸ ØªØ­Ø°ÙŠØ±: Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØª: {e}")
            return False
    
    def transcribe_audio(self):
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Whisper"""
        if not self.audio_path or not self.audio_path.exists():
            print("âš ï¸ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ù„Ù ØµÙˆØªÙŠ Ù„Ù„ØªØ­ÙˆÙŠÙ„")
            return None
        
        print("ğŸ¤ Ø¬Ø§Ø±ÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ (Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ø¨Ø¹Ø¶ Ø§Ù„ÙˆÙ‚Øª)...")
        try:
            import whisper
            
            # ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Whisper (base Ù„Ù„Ø³Ø±Ø¹Ø©)
            if self.whisper_model is None:
                print("ğŸ“¦ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Whisper...")
                self.whisper_model = whisper.load_model("base")
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ
            result = self.whisper_model.transcribe(
                str(self.audio_path),
                language="en",  # ÙŠÙ…ÙƒÙ† ØªØºÙŠÙŠØ±Ù‡ Ø­Ø³Ø¨ Ù„ØºØ© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
                verbose=False
            )
            
            print(f"âœ… ØªÙ… ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ ({len(result['segments'])} Ù…Ù‚Ø·Ø¹)")
            return result
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª: {e}")
            return None
    
    def analyze_frames(self, interval=1.0):
        """ØªØ­Ù„ÙŠÙ„ Ø¥Ø·Ø§Ø±Ø§Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨ÙØ§ØµÙ„ Ø²Ù…Ù†ÙŠ Ù…Ø­Ø¯Ø¯"""
        print(f"ğŸ¬ Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ø¥Ø·Ø§Ø±Ø§Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆ (ÙƒÙ„ {interval} Ø«Ø§Ù†ÙŠØ©)...")
        
        cap = cv2.VideoCapture(str(self.video_path))
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        print(f"  ğŸ“Š Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆ: {width}x{height} @ {fps:.2f} FPS, Ø§Ù„Ù…Ø¯Ø©: {timedelta(seconds=int(duration))}")
        
        frames_data = []
        frame_interval = int(fps * interval)
        
        frame_count = 0
        saved_count = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            if frame_count % frame_interval == 0:
                timestamp = frame_count / fps
                
                # Ø­ÙØ¸ Ø§Ù„Ø¥Ø·Ø§Ø±
                frame_filename = f"frame_{saved_count:04d}_{timestamp:.2f}s.jpg"
                frame_path = self.output_dir / "frames" / frame_filename
                frame_path.parent.mkdir(exist_ok=True)
                
                cv2.imwrite(str(frame_path), frame)
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø·Ø§Ø±
                frame_info = self.analyze_single_frame(frame, timestamp)
                frame_info['filename'] = frame_filename
                frame_info['path'] = str(frame_path)
                
                frames_data.append(frame_info)
                saved_count += 1
                
                if saved_count % 10 == 0:
                    print(f"  ğŸ“¸ ØªÙ… ØªØ­Ù„ÙŠÙ„ {saved_count} Ø¥Ø·Ø§Ø±...")
            
            frame_count += 1
        
        cap.release()
        print(f"âœ… ØªÙ… ØªØ­Ù„ÙŠÙ„ {len(frames_data)} Ø¥Ø·Ø§Ø±")
        
        return {
            'fps': fps,
            'total_frames': total_frames,
            'duration': duration,
            'resolution': f"{width}x{height}",
            'frames': frames_data
        }
    
    def analyze_single_frame(self, frame, timestamp):
        """ØªØ­Ù„ÙŠÙ„ Ø¥Ø·Ø§Ø± ÙˆØ§Ø­Ø¯"""
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©
        height, width, channels = frame.shape
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø³Ø·ÙˆØ¹ Ø§Ù„Ù…ØªÙˆØ³Ø·
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        brightness = np.mean(gray)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¨Ø§ÙŠÙ†
        contrast = np.std(gray)
        
        # Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø§Ù„Ø³Ø§Ø¦Ø¯Ø©
        avg_color = np.mean(frame, axis=(0, 1))
        
        # ÙƒØ´Ù Ø§Ù„Ø­ÙˆØ§Ù (Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ÙÙƒØ±Ø© Ø¹Ù† Ø§Ù„ØªØ¹Ù‚ÙŠØ¯)
        edges = cv2.Canny(gray, 100, 200)
        edge_density = np.sum(edges > 0) / (width * height)
        
        # ÙƒØ´Ù Ø§Ù„Ø­Ø±ÙƒØ© (Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ø¹ Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ø³Ø§Ø¨Ù‚)
        # ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ†Ù‡ Ù„Ø§Ø­Ù‚Ø§Ù‹
        
        return {
            'timestamp': timestamp,
            'time_formatted': str(timedelta(seconds=int(timestamp))),
            'resolution': f"{width}x{height}",
            'brightness': float(brightness),
            'contrast': float(contrast),
            'avg_color_bgr': [float(c) for c in avg_color],
            'edge_density': float(edge_density),
            'scene_complexity': 'high' if edge_density > 0.1 else 'medium' if edge_density > 0.05 else 'low',
            'visual_description': self.describe_frame(brightness, contrast, edge_density)
        }
    
    def describe_frame(self, brightness, contrast, edge_density):
        """ÙˆØµÙ Ø§Ù„Ø¥Ø·Ø§Ø± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø®ØµØ§Ø¦Øµ"""
        desc = []
        
        if brightness < 50:
            desc.append("Ù…Ø´Ù‡Ø¯ Ù…Ø¸Ù„Ù…")
        elif brightness > 200:
            desc.append("Ù…Ø´Ù‡Ø¯ Ø³Ø§Ø·Ø¹ Ø¬Ø¯Ø§Ù‹")
        else:
            desc.append("Ø¥Ø¶Ø§Ø¡Ø© Ù…ØªÙˆØ³Ø·Ø©")
        
        if contrast < 30:
            desc.append("ØªØ¨Ø§ÙŠÙ† Ù…Ù†Ø®ÙØ¶")
        elif contrast > 70:
            desc.append("ØªØ¨Ø§ÙŠÙ† Ø¹Ø§Ù„ÙŠ")
        
        if edge_density > 0.1:
            desc.append("ØªÙØ§ØµÙŠÙ„ ÙƒØ«ÙŠØ±Ø©")
        elif edge_density < 0.05:
            desc.append("Ù…Ø´Ù‡Ø¯ Ø¨Ø³ÙŠØ·")
        
        return ", ".join(desc)
    
    def generate_markdown_report(self, frames_analysis, transcription):
        """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Markdown Ø´Ø§Ù…Ù„"""
        print("ğŸ“ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Markdown...")
        
        md_content = []
        
        # Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
        md_content.append("# ğŸ“¹ ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ø´Ø§Ù…Ù„\n")
        md_content.append("# Video Analysis Report - Complete Training Material\n\n")
        md_content.append(f"---\n\n")
        
        # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ø§Ù…Ø©
        md_content.append("## ğŸ“Š Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ø¹Ø§Ù…Ø© / General Video Information\n\n")
        md_content.append(f"- **Ø§Ù„Ù…ØµØ¯Ø± / Source**: `{self.video_url}`\n")
        md_content.append(f"- **Ø§Ù„Ù…Ø¯Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ© / Total Duration**: {timedelta(seconds=int(frames_analysis['duration']))}\n")
        md_content.append(f"- **Ø§Ù„Ø¯Ù‚Ø© / Resolution**: {frames_analysis['resolution']}\n")
        md_content.append(f"- **Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª / FPS**: {frames_analysis['fps']:.2f}\n")
        md_content.append(f"- **Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª / Total Frames**: {frames_analysis['total_frames']}\n")
        md_content.append(f"- **Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø­Ù„Ù„Ø© / Analyzed Frames**: {len(frames_analysis['frames'])}\n\n")
        
        # Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬ Ù…Ù† Ø§Ù„ØµÙˆØª
        if transcription:
            md_content.append("## ğŸ¤ Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ Ù…Ù† Ø§Ù„ØµÙˆØª / Full Audio Transcription\n\n")
            md_content.append("### Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ / Complete Text:\n\n")
            md_content.append(f"```\n{transcription['text']}\n```\n\n")
            
            md_content.append("### Ø§Ù„Ù†Øµ Ù…Ù‚Ø³Ù… Ø­Ø³Ø¨ Ø§Ù„ÙˆÙ‚Øª / Time-Segmented Transcription:\n\n")
            for segment in transcription['segments']:
                start_time = str(timedelta(seconds=int(segment['start'])))
                end_time = str(timedelta(seconds=int(segment['end'])))
                md_content.append(f"**[{start_time} - {end_time}]**\n")
                md_content.append(f"> {segment['text'].strip()}\n\n")
        else:
            md_content.append("## ğŸ¤ Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ Ù…Ù† Ø§Ù„ØµÙˆØª / Full Audio Transcription\n\n")
            md_content.append("âš ï¸ **Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØª** (ffmpeg ØºÙŠØ± Ù…ØªÙˆÙØ± ÙÙŠ Ø§Ù„Ø¨ÙŠØ¦Ø©)\n\n")
            md_content.append("*Audio transcription not available (ffmpeg not installed)*\n\n")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø¨Ø§Ù„ØªÙØµÙŠÙ„
        md_content.append("## ğŸ¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø§Ù„Ø«Ø§Ù†ÙŠØ© / Frame-by-Frame Analysis\n\n")
        md_content.append("*ÙƒÙ„ Ø¥Ø·Ø§Ø± ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ ØªÙ‚Ù†ÙŠ Ù…ÙØµÙ„ Ù„Ù„ØµÙˆØ±Ø©*\n\n")
        md_content.append("*Each frame includes detailed technical image analysis*\n\n")
        
        for i, frame in enumerate(frames_analysis['frames']):
            md_content.append(f"### â±ï¸ Ø§Ù„Ø«Ø§Ù†ÙŠØ© {frame['timestamp']:.2f} ({frame['time_formatted']})\n\n")
            
            # Ø§Ù„ØµÙˆØ±Ø©
            md_content.append(f"![Frame {i} at {frame['time_formatted']}](frames/{frame['filename']})\n\n")
            
            # Ø§Ù„ÙˆØµÙ Ø§Ù„Ø¨ØµØ±ÙŠ
            md_content.append(f"**Ø§Ù„ÙˆØµÙ Ø§Ù„Ø¨ØµØ±ÙŠ / Visual Description**: {frame['visual_description']}\n\n")
            
            # Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„ØªÙ‚Ù†ÙŠØ©
            md_content.append("<details>\n")
            md_content.append("<summary><strong>Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„ØªÙ‚Ù†ÙŠØ© / Technical Details</strong> (Ø§Ù†Ù‚Ø± Ù„Ù„ØªÙˆØ³ÙŠØ¹ / Click to expand)</summary>\n\n")
            md_content.append(f"- **Ø§Ù„Ø¯Ù‚Ø© / Resolution**: {frame['resolution']}\n")
            md_content.append(f"- **Ø§Ù„Ø³Ø·ÙˆØ¹ / Brightness**: {frame['brightness']:.2f}/255\n")
            md_content.append(f"- **Ø§Ù„ØªØ¨Ø§ÙŠÙ† / Contrast**: {frame['contrast']:.2f}\n")
            md_content.append(f"- **ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù…Ø´Ù‡Ø¯ / Scene Complexity**: {frame['scene_complexity']}\n")
            md_content.append(f"- **ÙƒØ«Ø§ÙØ© Ø§Ù„Ø­ÙˆØ§Ù / Edge Density**: {frame['edge_density']:.4f}\n")
            md_content.append(f"- **Ù…ØªÙˆØ³Ø· Ø§Ù„Ù„ÙˆÙ† (BGR) / Avg Color**: [{frame['avg_color_bgr'][0]:.1f}, {frame['avg_color_bgr'][1]:.1f}, {frame['avg_color_bgr'][2]:.1f}]\n")
            md_content.append("\n</details>\n\n")
            
            # Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„ Ù…Ù† Ø§Ù„ØµÙˆØª
            if transcription:
                matching_text = self.find_matching_transcription(
                    frame['timestamp'], 
                    transcription['segments']
                )
                if matching_text:
                    md_content.append("**Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù†Ø·ÙˆÙ‚ ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù„Ø­Ø¸Ø© / Spoken Text at This Moment:**\n\n")
                    md_content.append(f"> ğŸ—£ï¸ {matching_text}\n\n")
            
            md_content.append("---\n\n")
        
        # Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        md_content.append("## ğŸ“ˆ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„ / Statistics & Analysis\n\n")
        
        avg_brightness = np.mean([f['brightness'] for f in frames_analysis['frames']])
        avg_contrast = np.mean([f['contrast'] for f in frames_analysis['frames']])
        avg_edge_density = np.mean([f['edge_density'] for f in frames_analysis['frames']])
        
        md_content.append("### Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¹Ø§Ù…Ø© / General Statistics\n\n")
        md_content.append(f"- **Ù…ØªÙˆØ³Ø· Ø§Ù„Ø³Ø·ÙˆØ¹ / Average Brightness**: {avg_brightness:.2f}/255\n")
        md_content.append(f"- **Ù…ØªÙˆØ³Ø· Ø§Ù„ØªØ¨Ø§ÙŠÙ† / Average Contrast**: {avg_contrast:.2f}\n")
        md_content.append(f"- **Ù…ØªÙˆØ³Ø· ÙƒØ«Ø§ÙØ© Ø§Ù„Ø­ÙˆØ§Ù / Average Edge Density**: {avg_edge_density:.4f}\n\n")
        
        complexity_counts = {}
        for f in frames_analysis['frames']:
            complexity = f['scene_complexity']
            complexity_counts[complexity] = complexity_counts.get(complexity, 0) + 1
        
        md_content.append("### ØªÙˆØ²ÙŠØ¹ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯ / Scene Complexity Distribution\n\n")
        for complexity, count in sorted(complexity_counts.items()):
            percentage = (count / len(frames_analysis['frames'])) * 100
            bar = "â–ˆ" * int(percentage / 2)
            md_content.append(f"- **{complexity.capitalize()}**: {count} Ø¥Ø·Ø§Ø± ({percentage:.1f}%) {bar}\n")
        
        md_content.append("\n---\n\n")
        md_content.append("## ğŸ’¡ Ù…Ù„Ø§Ø­Ø¸Ø§Øª / Notes\n\n")
        md_content.append("- ØªÙ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±\n")
        md_content.append("- ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ø§ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙƒÙ…Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ÙŠØ© Ø´Ø§Ù…Ù„Ø©\n")
        md_content.append("- Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ù…Ø­ÙÙˆØ¸Ø© ÙÙŠ Ù…Ø¬Ù„Ø¯ `frames/`\n\n")
        md_content.append("*This video was automatically analyzed using computer vision techniques*\n")
        md_content.append("*This report can be used as comprehensive training material*\n")
        md_content.append("*All frames are saved in the `frames/` directory*\n\n")
        
        md_content.append("---\n\n")
        md_content.append("*ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù‡Ø°Ø§ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¨ÙˆØ§Ø³Ø·Ø© Video Analyzer*\n\n")
        md_content.append("*Report automatically generated by Video Analyzer*\n")
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        report_path = self.output_dir / "video_analysis_report.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(''.join(md_content))
        
        print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {report_path}")
        return report_path
    
    def find_matching_transcription(self, timestamp, segments):
        """Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚ Ù„Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø­Ø¯Ø¯"""
        for segment in segments:
            if segment['start'] <= timestamp <= segment['end']:
                return segment['text'].strip()
        return None
    
    def analyze(self, frame_interval=1.0):
        """ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„"""
        print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ...\n")
        print("=" * 60)
        
        # 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
        if not self.download_video():
            return False
        
        # 2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØª
        audio_extracted = self.extract_audio_subprocess()
        
        # 3. ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ
        transcription = None
        if audio_extracted:
            transcription = self.transcribe_audio()
        
        # 4. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª
        frames_analysis = self.analyze_frames(interval=frame_interval)
        
        # 5. ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        report_path = self.generate_markdown_report(frames_analysis, transcription)
        
        # 6. Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù…
        data_path = self.output_dir / "analysis_data.json"
        with open(data_path, 'w', encoding='utf-8') as f:
            json.dump({
                'video_url': self.video_url,
                'frames_analysis': {
                    'fps': frames_analysis['fps'],
                    'total_frames': frames_analysis['total_frames'],
                    'duration': frames_analysis['duration'],
                    'resolution': frames_analysis['resolution'],
                    'frame_count': len(frames_analysis['frames'])
                },
                'has_transcription': transcription is not None
            }, f, ensure_ascii=False, indent=2)
        
        print("\n" + "=" * 60)
        print(f"âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­!")
        print(f"\nğŸ“ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙˆÙ„Ø¯Ø© / Generated Files:")
        print(f"   ğŸ“„ Ø§Ù„ØªÙ‚Ø±ÙŠØ± / Report: {report_path}")
        print(f"   ğŸ“Š Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… / Raw Data: {data_path}")
        print(f"   ğŸ–¼ï¸  Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª / Frames: {self.output_dir / 'frames'} ({len(frames_analysis['frames'])} files)")
        print("=" * 60)
        
        return True


def main():
    if len(sys.argv) < 2:
        print("=" * 60)
        print("Video Analyzer - ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ")
        print("=" * 60)
        print("\nØ§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… / Usage:")
        print("  python video_analyzer_enhanced.py <video_url> [frame_interval]")
        print("\nÙ…Ø«Ø§Ù„ / Example:")
        print('  python video_analyzer_enhanced.py "https://example.com/video.mp4" 1.0')
        print("\nØ§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª / Parameters:")
        print("  video_url       : Ø±Ø§Ø¨Ø· Ø§Ù„ÙÙŠØ¯ÙŠÙˆ / Video URL")
        print("  frame_interval  : Ø§Ù„ÙØ§ØµÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠ Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ (Ø§ÙØªØ±Ø§Ø¶ÙŠ: 1.0) / Time interval in seconds (default: 1.0)")
        print("=" * 60)
        sys.exit(1)
    
    video_url = sys.argv[1]
    frame_interval = float(sys.argv[2]) if len(sys.argv) > 2 else 1.0
    
    print(f"\nğŸ¯ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ / Analyzing Video:")
    print(f"   URL: {video_url}")
    print(f"   Ø§Ù„ÙØ§ØµÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠ / Frame Interval: {frame_interval}s\n")
    
    analyzer = VideoAnalyzer(video_url)
    success = analyzer.analyze(frame_interval=frame_interval)
    
    if success:
        print("\nâœ… ØªÙ… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­! / Analysis completed successfully!")
    else:
        print("\nâŒ ÙØ´Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ / Analysis failed!")
        sys.exit(1)


if __name__ == "__main__":
    main()
